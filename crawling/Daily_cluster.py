import psycopg2
import pandas as pd
import numpy as np
import json
import time
import logging
import re
import sys
import warnings
from datetime import datetime
from collections import Counter
from difflib import SequenceMatcher
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
from sqlalchemy import create_engine

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore")

# ==========================================
# 1. ì„¤ì •
# ==========================================

DB_CONFIG = {
    "host": "localhost",
    "dbname": "postgres",
    "user": "postgres",
    "password": "0000",
    "port": "5432"
}

CHECK_INTERVAL = 10  # ì‹¤í–‰ ì£¼ê¸° (ì´ˆ)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s | %(message)s', 
    datefmt='%H:%M:%S'
)

# SQLAlchemy ì—”ì§„
db_url = f"postgresql+psycopg2://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}"
engine = create_engine(db_url)

def get_db_connection():
    return psycopg2.connect(**DB_CONFIG)

def parse_embedding(emb_str):
    try:
        if isinstance(emb_str, str): return np.array(json.loads(emb_str))
        elif isinstance(emb_str, list): return np.array(emb_str)
        return np.zeros(1024)
    except: return np.zeros(1024)

def clean_text_for_title(text):
    text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
    return ' '.join(text.split())

# ==========================================
# 2. ê±°ë¦¬ ê³„ì‚° ë¡œì§
# ==========================================

def calculate_hybrid_distance(embeddings, keywords_list, alpha=0.6):
    n = len(embeddings)
    if n == 0: return np.zeros((0, 0))
    
    emb_sim = cosine_similarity(embeddings)
    key_sim = np.zeros((n, n))
    keyword_sets = [set(k) if k else set() for k in keywords_list]

    for i in range(n):
        for j in range(i, n):
            if i == j: key_sim[i][j] = 1.0; continue
            u_len = len(keyword_sets[i].union(keyword_sets[j]))
            sim = len(keyword_sets[i].intersection(keyword_sets[j])) / u_len if u_len > 0 else 0.0
            key_sim[i][j] = key_sim[j][i] = sim
            
    dist = 1 - ((emb_sim * alpha) + (key_sim * (1 - alpha)))
    dist[dist < 0] = 0
    return dist

def calculate_text_distance(texts):
    n = len(texts)
    dist_matrix = np.zeros((n, n))
    for i in range(n):
        for j in range(i, n):
            if i == j: dist_matrix[i][j] = 0.0; continue
            sim = SequenceMatcher(None, texts[i], texts[j]).ratio()
            dist_matrix[i][j] = dist_matrix[j][i] = 1.0 - sim
    return dist_matrix

# ==========================================
# 3. í•µì‹¬ ë¡œì§: ë³‘í•© & ì‹ ê·œ ìƒì„±
# ==========================================

def try_merge_to_existing_incidents(conn, new_df):
    """ê¸°ì¡´ ì‚¬ê±´ê³¼ ìœ ì‚¬í•˜ë©´ ë³‘í•© (CLOSEDëœ ì‚¬ê±´ì´ë¼ë„ ìœ ì‚¬í•˜ë©´ ë³‘í•© í›„ OPENìœ¼ë¡œ ë¶€í™œ ê°€ëŠ¥)"""
    cursor = conn.cursor()
    merged_ids = []
    
    # ìµœê·¼ 30ì¼ ì´ë‚´ì˜ í™œì„± ì‚¬ê±´ ì¡°íšŒ
    sql_active = """
        SELECT i.id as incident_id, i.district_id,
               n.embedding, n.keywords_jsonb
        FROM incidents i
        JOIN complaints c ON c.incident_id = i.id
        JOIN complaint_normalizations n ON n.complaint_id = c.id
        WHERE i.opened_at > NOW() - INTERVAL '30 days'
        -- ì¢…ê²°ëœ ì‚¬ê±´(CLOSED)ë„ ë³‘í•© ëŒ€ìƒì— í¬í•¨í• ì§€ ì—¬ë¶€ëŠ” ì •ì±…ì— ë”°ë¦„.
        -- ì—¬ê¸°ì„œëŠ” 'ìœ ì‚¬í•˜ë©´ ë³‘í•©'ì„ ìš°ì„ í•˜ì—¬ CLOSEDë„ í¬í•¨í•´ì„œ ê²€ìƒ‰ í›„, ë³‘í•© ì‹œ ìƒíƒœë¥¼ OPENìœ¼ë¡œ ë°”ê¿ˆ
        ORDER BY i.id, c.created_at ASC
    """
    
    try:
        full_active_df = pd.read_sql(sql_active, engine)
    except Exception as e:
        logging.error(f"ê¸°ì¡´ ì‚¬ê±´ ì¡°íšŒ ì¤‘ ì—ëŸ¬: {e}")
        return new_df

    if full_active_df.empty:
        return new_df

    active_incidents = full_active_df.drop_duplicates(subset=['incident_id']).reset_index(drop=True)
    logging.info(f"ğŸ” [ë¹„êµ] ê¸°ì¡´ ì‚¬ê±´ {len(active_incidents)}ê°œì™€ ìœ ì‚¬ë„ ë¶„ì„ ì¤‘...")

    for idx, row in new_df.iterrows():
        my_emb = parse_embedding(row['embedding']).reshape(1, -1)
        my_k = set(row['keywords_jsonb']) if row['keywords_jsonb'] else set()
        my_dist_id = row['district_id']

        candidates = active_incidents[active_incidents['district_id'] == my_dist_id]
        if candidates.empty: continue

        cand_embs = np.array([parse_embedding(e) for e in candidates['embedding']])
        if len(cand_embs) == 0: continue

        sim_scores = cosine_similarity(my_emb, cand_embs)[0]
        
        best_score = -1
        best_inc_id = None

        for i, score in enumerate(sim_scores):
            if score < 0.85: continue
            
            cand_k = set(candidates.iloc[i]['keywords_jsonb']) if candidates.iloc[i]['keywords_jsonb'] else set()
            if len(my_k.intersection(cand_k)) == 0: continue 

            if score > best_score:
                best_score = score
                best_inc_id = candidates.iloc[i]['incident_id']

        if best_inc_id and best_score >= 0.85:
            try:
                # 1. ë¯¼ì› ì—…ë°ì´íŠ¸
                cursor.execute("""
                    UPDATE complaints 
                    SET incident_id = %s, incident_linked_at = NOW(), incident_link_score = %s 
                    WHERE id = %s
                """, (int(best_inc_id), float(best_score), int(row['id'])))
                
                # 2. ì‚¬ê±´ ì—…ë°ì´íŠ¸ (ë¯¼ì› ìˆ˜ ì¦ê°€)
                # [ì¤‘ìš”] ì‹ ê·œ ë¯¼ì›ì´ ì¶”ê°€ë˜ë©´, í˜¹ì‹œ ì¢…ê²°(CLOSED)ë˜ì—ˆë˜ ì‚¬ê±´ë„ ë‹¤ì‹œ ëŒ€ì‘ì¤‘(OPEN)ìœ¼ë¡œ ë°”ë€Œì–´ì•¼ í•¨
                cursor.execute("""
                    UPDATE incidents 
                    SET complaint_count = complaint_count + 1,
                        status = 'OPEN' 
                    WHERE id = %s
                """, (int(best_inc_id),))
                
                logging.info(f"  ğŸ”— [ë³‘í•© ì„±ê³µ] ë¯¼ì› #{row['id']} -> ì‚¬ê±´ #{best_inc_id} (ì ìˆ˜: {best_score:.2f})")
                merged_ids.append(row['id'])
            except Exception as e:
                logging.error(f"  âŒ ë³‘í•© ì‹¤íŒ¨: {e}")

    conn.commit()
    cursor.close()
    
    return new_df[~new_df['id'].isin(merged_ids)]

def cluster_remaining_complaints(conn, df):
    if df.empty: return

    logging.info(f"ğŸ§© [ì‹ ê·œ êµ°ì§‘í™”] ë‚¨ì€ ë¯¼ì› {len(df)}ê±´ ì²˜ë¦¬ ì¤‘...")
    cursor = conn.cursor()
    
    df['district_id'] = df['district_id'].fillna(0)
    grouped = df.groupby('district_id')

    for dist_id, group in grouped:
        if len(group) == 0: continue
        
        if len(group) == 1:
            save_incident(cursor, group, is_noise=True)
            continue

        embeddings = np.array([parse_embedding(e) for e in group['embedding']])
        keywords_list = [k if k else [] for k in group['keywords_jsonb'].tolist()]
        
        l1_dist = calculate_hybrid_distance(embeddings, keywords_list, alpha=0.6)
        l1_labels = DBSCAN(eps=0.15, min_samples=2, metric='precomputed').fit_predict(l1_dist)

        for l1_lab in set(l1_labels):
            l1_indices = np.where(l1_labels == l1_lab)[0]
            l1_df = group.iloc[l1_indices]

            if l1_lab == -1: 
                save_incident(cursor, l1_df, is_noise=True)
            else:
                save_incident(cursor, l1_df, is_noise=False)

    conn.commit()
    cursor.close()

def save_incident(cursor, df, is_noise=False):
    if df.empty: return

    iterator = df.iterrows() if is_noise else [(None, df)]
    
    for _, row_data in iterator:
        if is_noise:
            target_df = pd.DataFrame([row_data])
            row_item = row_data
            count = 1
        else:
            target_df = row_data
            row_item = target_df.iloc[0]
            count = len(target_df)

        dist_name = row_item['district_name'] if row_item['district_name'] else "ì„œìš¸ì‹œ"
        
        all_k = []
        for k_list in target_df['keywords_jsonb']:
            if k_list: all_k.extend(k_list)
        top_k = Counter(all_k).most_common(5)
        
        main_keyword = top_k[0][0] if top_k else "ë¯¼ì›"
        keywords_str = ", ".join([k[0] for k in top_k]) 
        
        valid_reqs = [r for r in target_df['core_request'].tolist() if r]
        raw_summ = max(valid_reqs, key=len) if valid_reqs else "ë‚´ìš© ì—†ìŒ"
        
        title = f"{dist_name} {main_keyword} ê´€ë ¨ {raw_summ}"
        title = clean_text_for_title(title)[:100].strip()

        d_id = int(row_item['district_id']) if row_item['district_id'] > 0 else None
        
        try:
            # [ë³€ê²½] ì´ˆê¸° ìƒíƒœëŠ” ë¬´ì¡°ê±´ 'OPEN' (ëŒ€ì‘ì¤‘)
            cursor.execute("""
                INSERT INTO incidents (title, status, complaint_count, keywords, district_id, opened_at)
                VALUES (%s, 'OPEN', %s, %s, %s, NOW())
                RETURNING id
            """, (title, count, keywords_str, d_id))
            inc_id = cursor.fetchone()[0]

            ids = tuple(target_df['id'].tolist())
            cursor.execute(f"""
                UPDATE complaints 
                SET incident_id = %s, incident_linked_at = NOW(), incident_link_score = 0.95 
                WHERE id IN %s
            """, (inc_id, ids))
            
            logging.info(f"  ğŸ†• [ì‚¬ê±´ ìƒì„±] #{inc_id} : {title} ({count}ê±´)")
        except Exception as e:
            logging.error(f"  âŒ ì‚¬ê±´ ì €ì¥ ì‹¤íŒ¨: {e}")

# ==========================================
# 5. [ìˆ˜ì •ë¨] ìƒíƒœ ë™ê¸°í™” í•¨ìˆ˜ (2ë‹¨ê³„ ë¡œì§)
# ==========================================
def sync_incident_status(conn):
    """
    ë¯¼ì› ìƒíƒœì— ë”°ë¥¸ ì‚¬ê±´ ìƒíƒœ ë™ê¸°í™” (ë‹¨ìˆœí™”ëœ ë¡œì§)
    
    1. CLOSED (ì¢…ê²°): ëª¨ë“  ë¯¼ì›ì´ 'CLOSED' ë˜ëŠ” 'CANCELED'ì¸ ê²½ìš°
    2. OPEN (ëŒ€ì‘ì¤‘): í•˜ë‚˜ë¼ë„ ëë‚˜ì§€ ì•Šì€ ë¯¼ì›('RECEIVED', 'IN_PROGRESS' ë“±)ì´ ìˆëŠ” ê²½ìš°
    """
    cursor = conn.cursor()
    try:
        # 1. [ì¢…ê²° ì²˜ë¦¬] (OPEN -> CLOSED)
        # ì¡°ê±´: í˜„ì¬ OPENì¸ë°, ì†Œì†ëœ ëª¨ë“  ë¯¼ì›ì´ (CLOSED or CANCELED) ìƒíƒœì¼ ë•Œ
        cursor.execute("""
            UPDATE incidents i
            SET status = 'CLOSED', closed_at = NOW()
            WHERE i.status = 'OPEN'
            AND NOT EXISTS (
                SELECT 1 FROM complaints c 
                WHERE c.incident_id = i.id 
                AND c.status NOT IN ('CLOSED', 'CANCELED')
            )
            AND EXISTS (SELECT 1 FROM complaints c WHERE c.incident_id = i.id)
        """)
        if cursor.rowcount > 0:
            logging.info(f"  ğŸ [ìƒíƒœ ë™ê¸°í™”] {cursor.rowcount}ê°œ ì‚¬ê±´ -> 'ì¢…ê²°(CLOSED)'ë¡œ ë³€ê²½")

        # 2. [ëŒ€ì‘ì¤‘ ë³µêµ¬] (CLOSED -> OPEN)
        # ì¡°ê±´: í˜„ì¬ CLOSEDì¸ë°, ëë‚˜ì§€ ì•Šì€ ë¯¼ì›ì´ í•˜ë‚˜ë¼ë„ ìƒê²¼ì„ ë•Œ (ì¬ì ‘ìˆ˜, ì‹ ê·œë³‘í•© ë“±)
        cursor.execute("""
            UPDATE incidents i
            SET status = 'OPEN', closed_at = NULL
            WHERE i.status = 'CLOSED'
            AND EXISTS (
                SELECT 1 FROM complaints c 
                WHERE c.incident_id = i.id 
                AND c.status NOT IN ('CLOSED', 'CANCELED')
            )
        """)
        if cursor.rowcount > 0:
            logging.info(f"  ğŸ”„ [ìƒíƒœ ë™ê¸°í™”] {cursor.rowcount}ê°œ ì‚¬ê±´ -> 'ëŒ€ì‘ì¤‘(OPEN)'ìœ¼ë¡œ ë³µêµ¬")

        conn.commit()
    except Exception as e:
        logging.error(f"ìƒíƒœ ë™ê¸°í™” ì¤‘ ì—ëŸ¬: {e}")
        conn.rollback()
    finally:
        cursor.close()

# ==========================================
# 6. ì‹¤í–‰ ë£¨í”„
# ==========================================

def run_daily_job():
    conn = get_db_connection()
    try:
        sql = """
            SELECT n.complaint_id as id, n.core_request, n.embedding,
                   n.keywords_jsonb, n.district_id, n.target_object, 
                   d.name as district_name
            FROM complaint_normalizations n
            JOIN complaints c ON n.complaint_id = c.id
            LEFT JOIN districts d ON n.district_id = d.id
            WHERE c.incident_id IS NULL 
        """
        
        try:
            new_df = pd.read_sql(sql, engine)
        except Exception as e:
            logging.error(f"ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return

        if not new_df.empty:
            logging.info(f"âš¡ ì‹ ê·œ ë¯¼ì› {len(new_df)}ê±´ ê°ì§€! ë¶„ì„ ì‹œì‘...")
            
            remaining_df = try_merge_to_existing_incidents(conn, new_df)
            
            if not remaining_df.empty:
                cluster_remaining_complaints(conn, remaining_df)
                
            logging.info("âœ… ë¶„ì„ ë° ì²˜ë¦¬ ì™„ë£Œ.")
        
        # ë°ì´í„° ìœ ë¬´ì™€ ìƒê´€ì—†ì´ í•­ìƒ ìƒíƒœ ë™ê¸°í™” ìˆ˜í–‰
        sync_incident_status(conn)

    except Exception as e:
        conn.rollback()
        logging.error(f"âŒ ì „ì²´ ë¡œì§ ì—ëŸ¬: {e}")
    finally:
        conn.close()

def print_progress_bar(duration):
    width = 30
    for i in range(duration):
        time.sleep(1)
        progress = int((i + 1) / duration * width)
        bar = 'â–ˆ' * progress + '-' * (width - progress)
        sys.stdout.write(f"\râ³ ëŒ€ê¸° ì¤‘... [{bar}] {duration - i - 1}ì´ˆ ")
        sys.stdout.flush()
    sys.stdout.write("\r" + " " * 80 + "\r") 

if __name__ == "__main__":
    print("\n" + "="*50)
    print("ğŸ¤– [Daily Cluster] ì‹¤ì‹œê°„ ë¯¼ì› êµ°ì§‘í™” ê°€ë™")
    print("   - ëª¨ë“œ: 2ë‹¨ê³„ ìƒíƒœ ê´€ë¦¬ (OPEN / CLOSED)")
    print(f"   - ì£¼ê¸°: {CHECK_INTERVAL}ì´ˆ")
    print("="*50 + "\n")

    while True:
        run_daily_job()
        print_progress_bar(CHECK_INTERVAL)