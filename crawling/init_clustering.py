import psycopg2
import pandas as pd
import numpy as np
import json
import logging
import re
from collections import Counter
from datetime import datetime
from difflib import SequenceMatcher
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import silhouette_score



# ==========================================
# 1. ÏÑ§Ï†ï
# ==========================================

DB_CONFIG = {
    "host": "localhost",
    "dbname": "postgres",
    "user": "postgres",
    "password": "0000",
    "port": "5432"
}

logging.basicConfig(level=logging.INFO, format='üöÄ %(message)s')


# ÎåÄÌòï Íµ∞Ïßë Í∏∞Ï§Ä
LARGE_CLUSTER_THRESHOLD = 30


def get_db_connection():
    return psycopg2.connect(**DB_CONFIG)


def parse_embedding(emb_str):
    try:
        if isinstance(emb_str, str): return np.array(json.loads(emb_str))
        elif isinstance(emb_str, list): return np.array(emb_str)
        return np.zeros(1024)
    except: return np.zeros(1024)


# Ï†úÎ™© Ï†ïÏ†ú
def clean_text_for_title(text):
    text = re.sub(r'[^\w\sÍ∞Ä-Ìû£]', ' ', text)
    return ' '.join(text.split())


# ÌïòÏù¥Î∏åÎ¶¨Îìú Í±∞Î¶¨ Í≥ÑÏÇ∞
def calculate_hybrid_distance(embeddings, keywords_list, alpha=0.6):
    n = len(embeddings)
    emb_sim = cosine_similarity(embeddings)
    key_sim = np.zeros((n, n))
    keyword_sets = [set(k) if k else set() for k in keywords_list]
   

    for i in range(n):
        for j in range(i, n):
            if i == j: key_sim[i][j] = 1.0; continue
            u_len = len(keyword_sets[i].union(keyword_sets[j]))
            sim = len(keyword_sets[i].intersection(keyword_sets[j])) / u_len if u_len > 0 else 0.0
            key_sim[i][j] = key_sim[j][i] = sim
           

    dist = 1 - ((emb_sim * alpha) + (key_sim * (1 - alpha)))

    dist[dist < 0] = 0

    return dist


# ÌÖçÏä§Ìä∏ Í±∞Î¶¨ Í≥ÑÏÇ∞ (Level 3)
def calculate_text_distance(texts):
    n = len(texts)
    dist_matrix = np.zeros((n, n))

    for i in range(n):
        for j in range(i, n):
            if i == j:
                dist_matrix[i][j] = 0.0
                continue

            sim = SequenceMatcher(None, texts[i], texts[j]).ratio()
            dist = 1.0 - sim
            dist_matrix[i][j] = dist_matrix[j][i] = dist

    return dist_matrix


# ==========================================
# 2. Î©îÏù∏ Î°úÏßÅ
# ==========================================

def main():
    conn = get_db_connection()
    cursor = conn.cursor()

    print(f"üöÄ [Start] 3Îã®Í≥Ñ Ï†ïÎ∞Ä ÌïÑÌÑ∞ÎßÅ(Text Deep Check) Íµ∞ÏßëÌôî ({datetime.now()})")

    try:
        # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
        sql = """
            SELECT n.complaint_id as id, n.core_request, n.embedding,
                   n.keywords_jsonb, n.district_id, n.target_object, d.name as district_name
            FROM complaint_normalizations n
            JOIN complaints c ON n.complaint_id = c.id
            LEFT JOIN districts d ON n.district_id = d.id
            WHERE c.incident_id IS NULL AND n.is_current = true
        """

        df = pd.read_sql(sql, conn)
        if df.empty: return

        # Ï†ÑÏ≤òÎ¶¨
        df['district_id'] = df['district_id'].fillna(0)
        df['target_object'] = df['target_object'].fillna('Í∏∞ÌÉÄ')
        df['district_name'] = df['district_name'].fillna('ÏÑúÏö∏Ïãú')

        # 3Îã®Í≥Ñ Íµ∞ÏßëÌôî
        grouped = df.groupby(['district_id', 'target_object'])

       
        total_clusters = 0
        total_noise = 0

        scores = []


        for (dist_id, target), group in grouped:
            if len(group) < 2:
                save_incident(cursor, group, is_noise=True)
                total_noise += 1

                continue



            # === Level 1: ÌïòÏù¥Î∏åÎ¶¨Îìú Íµ∞ÏßëÌôî ===
            embeddings = np.array([parse_embedding(e) for e in group['embedding']])
            keywords_list = [k if k else [] for k in group['keywords_jsonb'].tolist()]
           
            l1_dist = calculate_hybrid_distance(embeddings, keywords_list, alpha=0.6)
            l1_labels = DBSCAN(eps=0.11, min_samples=2, metric='precomputed').fit_predict(l1_dist)
           

            for l1_lab in set(l1_labels):
                if l1_lab == -1:
                    sub_df = group.iloc[np.where(l1_labels == -1)[0]]
                    save_incident(cursor, sub_df, is_noise=True)
                    total_noise += len(sub_df)
                    continue

               

                l1_indices = np.where(l1_labels == l1_lab)[0]
                l1_df = group.iloc[l1_indices]
                final_groups_to_save = []



                # === Level 2: ÎåÄÌòï Íµ∞Ïßë Î∂ÑÌï† ===
                if len(l1_df) >= LARGE_CLUSTER_THRESHOLD:
                    l2_emb = embeddings[l1_indices]
                    l2_kw = [keywords_list[i] for i in l1_indices]
                    l2_dist = calculate_hybrid_distance(l2_emb, l2_kw, alpha=0.5)
                    l2_labels = DBSCAN(eps=0.17, min_samples=2, metric='precomputed').fit_predict(l2_dist)

                

                    for l2_lab in set(l2_labels):
                        l2_sub_indices = np.where(l2_labels == l2_lab)[0]
                        l2_df = l1_df.iloc[l2_sub_indices]
                       
                        if l2_lab == -1:
                            save_incident(cursor, l2_df, is_noise=True)
                            total_noise += len(l2_df)

                        else:
                            final_groups_to_save.append(l2_df)

                else:
                    final_groups_to_save.append(l1_df)



                # === Level 3: ÌÖçÏä§Ìä∏ ÏµúÏ¢Ö ÌïÑÌÑ∞ÎßÅ ===
                for candidate_df in final_groups_to_save:
                    if len(candidate_df) < 2:
                        save_incident(cursor, candidate_df, is_noise=True)

                        continue

                    texts = candidate_df['core_request'].tolist()
                    text_dist_matrix = calculate_text_distance(texts)
               
                    l3_labels = DBSCAN(eps=0.25, min_samples=2, metric='precomputed').fit_predict(text_dist_matrix)

                 
                    # [ÏàòÏ†ïÎêú Ï†ïÌôïÎèÑ Ï∏°Ï†ï Î°úÏßÅ] ÏóêÎü¨ Î∞©ÏßÄÏö© ÏïàÏ†ÑÏû•Ïπò Ï∂îÍ∞Ä
                    try:
                        valid_mask = l3_labels != -1
                        unique_core_labels = set(l3_labels[valid_mask])
                      
                        # ÌïµÏã¨ ÏàòÏ†ï: ÎÖ∏Ïù¥Ï¶àÎ•º Î∫Ä 'ÏßÑÏßú Íµ∞Ïßë'Ïù¥ 2Í∞ú Ïù¥ÏÉÅÏùº ÎïåÎßå Ï†êÏàò Í≥ÑÏÇ∞ Í∞ÄÎä•
                        # (Scikit-Learn ÎùºÏù¥Î∏åÎü¨Î¶¨Ïùò ÌïÑÏàò Ï°∞Í±¥)
                        if len(unique_core_labels) >= 2 and np.sum(valid_mask) >= 2:
                            score = silhouette_score(text_dist_matrix[valid_mask][:, valid_mask], l3_labels[valid_mask], metric='precomputed')
                            scores.append(score)

                    except Exception as e:

                        pass # Ï†êÏàò Í≥ÑÏÇ∞ Ïã§Ìå®Ìï¥ÎèÑ ÌîÑÎ°úÏÑ∏Ïä§Îäî Î©àÏ∂îÏßÄ ÏïäÏùå

                    for l3_lab in set(l3_labels):
                        l3_indices = np.where(l3_labels == l3_lab)[0]
                        final_df = candidate_df.iloc[l3_indices]

                      
                        if l3_lab == -1:
                            save_incident(cursor, final_df, is_noise=True)
                            total_noise += len(final_df)

                        else:
                            save_incident(cursor, final_df, is_noise=False)
                            total_clusters += 1

        conn.commit()

       

        # === ÏµúÏ¢Ö Î¶¨Ìè¨Ìä∏ ===

        avg_score = sum(scores) / len(scores) if scores else 0
        print("\n" + "="*50)
        print(f"üìä [ÏµúÏ¢Ö Íµ∞ÏßëÌôî ÏÑ±Ï†ÅÌëú]")
        print(f"‚úÖ ÏÉùÏÑ±Îêú ÏÇ¨Í±¥(Incidents): {total_clusters}Í∞ú")
        print(f"üßπ Í±∏Îü¨ÏßÑ Îã®ÎèÖÎØºÏõê(Noise): {total_noise}Í∞ú")
        print(f"üéØ ÌèâÍ∑† Ï†ïÌôïÎèÑ(Silhouette Score): {avg_score:.4f}")

       

        if avg_score > 0.5: print("   üåü [ÌåêÏ†ï] ÏïÑÏ£º ÌõåÎ•≠Ìï©ÎãàÎã§! (Íµ∞ÏßëÎì§Ïù¥ ÏïÑÏ£º Îã®Îã®ÌïòÍ≤å Î≠âÏ≥§Ïùå)")

        elif avg_score > 0.3: print("   ‚ú® [ÌåêÏ†ï] ÏñëÌò∏Ìï©ÎãàÎã§. (Îã§ÏñëÌïú ÎØºÏõêÏù¥ Ïûò Î∂ÑÎ•òÎê®)")

        else: print("   ‚ö†Ô∏è [ÌåêÏ†ï] Íµ∞Ïßë Ï†êÏàòÎäî ÎÇÆÏßÄÎßå, Ïù¥Îäî 1Í∞úÏùò ÎåÄÌòï Íµ∞ÏßëÏúºÎ°ú ÏôÑÎ≤ΩÌûà Î¨∂ÏòÄÍ±∞ÎÇò Îç∞Ïù¥ÌÑ∞Í∞Ä ÌååÌé∏ÌôîÎêú Í≤ΩÏö∞Ïùº Ïàò ÏûàÏäµÎãàÎã§.")

        print("="*50 + "\n")


    except Exception as e:
        conn.rollback()
        print(f"‚ùå ÏóêÎü¨ Î∞úÏÉù: {e}")

    finally:
        cursor.close()
        conn.close()

def save_incident(cursor, df, is_noise=False):
    if is_noise:
        iterator = df.iterrows()
        count_per_incident = 1
    else:
        iterator = [(None, df)]
        count_per_incident = len(df)

    for _, row_data in iterator:
        if is_noise:
            target_df = pd.DataFrame([row_data])
            row_item = row_data
        else:
            target_df = row_data
            row_item = target_df.iloc[0]

        # 1. ÏßÄÏó≠Î™Ö (Ïòà: Í∞ïÎèôÍµ¨)
        dist_name = row_item['district_name']
        
        # 2. ÌïµÏã¨ ÌÇ§ÏõåÎìú Ï∂îÏ∂ú (Í∞ÄÏû• ÎßéÏù¥ Ïñ∏Í∏âÎêú Îã®Ïñ¥ 1Í∞ú ÏÑ†Ï†ï)
        all_k = []
        for k_list in target_df['keywords_jsonb']:
            if k_list: all_k.extend(k_list)
            
        # ÌÇ§ÏõåÎìúÍ∞Ä ÏûàÎã§Î©¥ ÏµúÎπàÍ∞í 1Í∞úÎ•º ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ 'ÎØºÏõê'ÏúºÎ°ú ÎåÄÏ≤¥
        top_k_list = [k for k, v in Counter(all_k).most_common(1)]
        main_keyword = top_k_list[0] if top_k_list else "ÎØºÏõê"

        # 3. ÎåÄÌëú Î¨∏Ïû• ÏÑ†Ï†ï (Ï†ïÎ≥¥ÎüâÏù¥ Í∞ÄÏû• ÎßéÏùÄ Í∏¥ Î¨∏Ïû•ÏùÑ ÏÑ†ÌÉù)
        # core_request(ÌïµÏã¨ ÏöîÍµ¨ÏÇ¨Ìï≠)Í∞Ä ÎÇ¥Ïö©ÏùÑ Í∞ÄÏû• Ïûò ÏöîÏïΩÌïòÍ≥† ÏûàÏúºÎØÄÎ°ú Ïù¥Î•º ÏÇ¨Ïö©
        valid_requests = [r for r in target_df['core_request'].tolist() if r]
        if valid_requests:
            raw_summ = max(valid_requests, key=len)
        else:
            raw_summ = "ÎÇ¥Ïö© ÏóÜÏùå"
        
        # 4. Ï†úÎ™© Ï°∞Î¶Ω (Í∞ÄÎèÖÏÑ± ÏµúÏö∞ÏÑ†)
        # ÌòïÏãù: "Í∞ïÎèôÍµ¨ Ïû¨Í±¥Ï∂ï Í¥ÄÎ†® ÎëîÏ¥åÏ£ºÍ≥µ ÏïÑÌååÌä∏ ÏÜåÏùå ÌîºÌï¥ Ïã†Í≥† ÏöîÏ≤≠"
        temp_title = f"{dist_name} {main_keyword} Í¥ÄÎ†® {raw_summ}"

        # 5. ÌäπÏàòÎ¨∏Ïûê Ï†úÍ±∞ Î∞è Ï†ïÏ†ú (Í∏∞Ï°¥ Ìï®Ïàò ÌôúÏö©)
        # ÌäπÏàòÎ¨∏ÏûêÎäî Ï†úÍ±∞ÎêòÍ≥† ÎùÑÏñ¥Ïì∞Í∏∞ Í∏∞Ï§ÄÏúºÎ°ú Ï†ïÎ¶¨Îê©ÎãàÎã§.
        final_title = clean_text_for_title(temp_title)

        # 6. Í∏∏Ïù¥ Ï†úÌïú Î∞è ÎßàÍ∞ê Ï≤òÎ¶¨ (DB Ï†ÄÏû• ÌïúÍ≥Ñ Í≥†Î†§)
        # Îí§Ïóê ÏûòÎ¶¨Îäî '...' ÏóÜÏù¥ ÍπîÎÅîÌïòÍ≤å 150Ïûê ÏÑ†ÏóêÏÑú Ïª∑ (DB Ïª¨Îüº Ïó¨Ïú† Í≥†Î†§)
        final_title = final_title[:150].strip()
            
        d_id = int(row_item['district_id']) if row_item['district_id'] > 0 else None
        
        # 7. DB Ï†ÄÏû•
        cursor.execute("""
            INSERT INTO incidents (title, status, complaint_count, keywords, district_id, opened_at)
            VALUES (%s, 'OPEN', %s, %s, %s, NOW())
            RETURNING id
        """, (final_title, count_per_incident, main_keyword, d_id))
        inc_id = cursor.fetchone()[0]
      
        ids = tuple(target_df['id'].tolist())
        cursor.execute(f"""
            UPDATE complaints
            SET incident_id = %s, incident_linked_at = NOW(), incident_link_score = 0.95
            WHERE id IN %s
        """, (inc_id, ids))


if __name__ == "__main__":

    main()