import psycopg2
import pandas as pd
import numpy as np
import json
import time
import logging
import re
import sys
import warnings
from datetime import datetime
from collections import Counter
from difflib import SequenceMatcher
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
from sqlalchemy import create_engine

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore")

# ==========================================
# 1. ì„¤ì •
# ==========================================

DB_CONFIG = {
    "host": "db",
    "dbname": "postgres",
    "user": "postgres",
    "password": "0000",
    "port": "5432"
}

# [ì„¤ì •] ì‹¤í–‰ ì£¼ê¸° ë° ì„ê³„ê°’
CHECK_INTERVAL = 30         # ì‹¤í–‰ ì£¼ê¸° (ì´ˆ)
HYBRID_THRESHOLD = 0.65     # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í•©ê²© ì ìˆ˜ (0~1 ì‚¬ì´, ë†’ì„ìˆ˜ë¡ ì—„ê²©)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s | %(message)s', 
    datefmt='%H:%M:%S'
)

# SQLAlchemy ì—”ì§„
db_url = f"postgresql+psycopg2://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}"
engine = create_engine(db_url)

def get_db_connection():
    return psycopg2.connect(**DB_CONFIG)

def parse_embedding(emb_str):
    try:
        if isinstance(emb_str, str): return np.array(json.loads(emb_str))
        elif isinstance(emb_str, list): return np.array(emb_str)
        return np.zeros(1024)
    except: return np.zeros(1024)

def clean_text_for_title(text):
    text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
    return ' '.join(text.split())

# ==========================================
# 2. ê±°ë¦¬ ê³„ì‚° ë¡œì§ (ì‹ ê·œ êµ°ì§‘ ìƒì„±ìš©)
# ==========================================

def calculate_hybrid_distance(embeddings, keywords_list, alpha=0.6):
    n = len(embeddings)
    if n == 0: return np.zeros((0, 0))
    
    emb_sim = cosine_similarity(embeddings)
    key_sim = np.zeros((n, n))
    keyword_sets = [set(k) if k else set() for k in keywords_list]

    for i in range(n):
        for j in range(i, n):
            if i == j: key_sim[i][j] = 1.0; continue
            u_len = len(keyword_sets[i].union(keyword_sets[j]))
            sim = len(keyword_sets[i].intersection(keyword_sets[j])) / u_len if u_len > 0 else 0.0
            key_sim[i][j] = key_sim[j][i] = sim
            
    dist = 1 - ((emb_sim * alpha) + (key_sim * (1 - alpha)))
    dist[dist < 0] = 0
    return dist

# ==========================================
# 3. í•µì‹¬ ë¡œì§: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë³‘í•© (íŒ€ì› ì½”ë“œ ì ìš©)
# ==========================================

def try_merge_to_existing_incidents_hybrid(conn, new_df):
    """
    íŒ€ì›ë¶„ì˜ SQL ì•„ì´ë””ì–´ë¥¼ ì ìš©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í•¨ìˆ˜.
    Python ë°˜ë³µë¬¸ ëŒ€ì‹  DB ì¿¼ë¦¬ë¡œ ìµœì ì˜ ì‚¬ê±´ì„ ì°¾ìŠµë‹ˆë‹¤.
    """
    cursor = conn.cursor()
    merged_ids = []
    
    logging.info(f"ğŸ” [í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰] ì‹ ê·œ ë¯¼ì› {len(new_df)}ê±´ì„ DB ì—”ì§„ìœ¼ë¡œ ì •ë°€ ëŒ€ì¡°í•©ë‹ˆë‹¤.")

    # ------------------------------------------------------------------
    # [SQL ì„¤ëª…] 
    # 1. v_score: pgvectorì˜ ì½”ì‚¬ì¸ ê±°ë¦¬ (1 - ê±°ë¦¬ = ìœ ì‚¬ë„)
    # 2. k_score: JSONB í‚¤ì›Œë“œê°€ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸ (êµì§‘í•© ê°œìˆ˜)
    # 3. bonus: ì§€ì—­êµ¬ê°€ ê°™ìœ¼ë©´ ê°€ì‚°ì  (+0.2)
    # ------------------------------------------------------------------
    hybrid_search_sql = """
    WITH existing_incidents AS (
        SELECT 
            i.id AS incident_id,
            i.title,
            n.embedding,
            n.keywords_jsonb,
            n.district_id
        FROM incidents i
        JOIN complaints c ON c.incident_id = i.id
        JOIN complaint_normalizations n ON n.complaint_id = c.id
        WHERE i.status = 'OPEN' 
          AND i.opened_at > NOW() - INTERVAL '5 years'
          AND n.embedding IS NOT NULL
    ),
    scores AS (
        SELECT 
            incident_id,
            title,
            -- [1] ë²¡í„° ìœ ì‚¬ë„ (ë¹„ì¤‘ 0.6)
            (1 - (embedding <=> %s::vector)) AS v_score,
            
            -- [2] í‚¤ì›Œë“œ ìœ ì‚¬ë„ (ë¹„ì¤‘ 0.2)
            (SELECT COUNT(*) 
             FROM jsonb_array_elements_text(keywords_jsonb) k 
             WHERE k = ANY(%s::text[])) * 0.1 AS k_score,
             
            -- [3] ë³´ë„ˆìŠ¤ (ë¹„ì¤‘ 0.2)
            CASE WHEN district_id = %s THEN 0.2 ELSE 0 END AS bonus
        FROM existing_incidents
    )
    SELECT 
        incident_id, 
        title, 
        (v_score * 0.6 + k_score + bonus) AS final_score,
        v_score, k_score, bonus
    FROM scores
    WHERE (v_score * 0.6 + k_score + bonus) > %s
    ORDER BY final_score DESC
    LIMIT 1;
    """

    for idx, row in new_df.iterrows():
        # íŒŒë¼ë¯¸í„° ì¤€ë¹„
        # 1. ë²¡í„°: ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (PostgreSQL vector í˜•ì‹)
        emb_val = row['embedding']
        if isinstance(emb_val, str): emb_val = json.loads(emb_val)
        emb_str = str(emb_val).replace(' ', '') # ê³µë°± ì œê±° ë“± í¬ë§·íŒ…
        
        # 2. í‚¤ì›Œë“œ: ë¦¬ìŠ¤íŠ¸ (PostgreSQL ë°°ì—´ë¡œ ë³€í™˜)
        my_keywords = row['keywords_jsonb'] if row['keywords_jsonb'] else []
        
        # 3. ì§€ì—­êµ¬ ID
        my_dist_id = int(row['district_id']) if row['district_id'] > 0 else 0

        # ë¡œê·¸ìš© ID
        my_id = row['id']

        try:
            # SQL ì‹¤í–‰ (Threshold ê°’ ì „ë‹¬)
            cursor.execute(hybrid_search_sql, (emb_str, my_keywords, my_dist_id, HYBRID_THRESHOLD))
            result = cursor.fetchone()

            if result:
                best_inc_id, best_title, final_score, v, k, b = result
                
                print(f"   ğŸ‘‰ [ë§¤ì¹­ ì„±ê³µ] ì‚¬ê±´ #{best_inc_id} ('{best_title[:15]}...')")
                print(f"      - ìµœì¢… ì ìˆ˜: {final_score:.4f} (ê¸°ì¤€: {HYBRID_THRESHOLD})")
                print(f"      - ìƒì„¸: ë²¡í„°({v:.2f}) + í‚¤ì›Œë“œ({k:.2f}) + ë³´ë„ˆìŠ¤({b:.2f})")

                # DB ì—…ë°ì´íŠ¸ (ë³‘í•©)
                cursor.execute("""
                    UPDATE complaints 
                    SET incident_id = %s, incident_linked_at = NOW(), incident_link_score = %s 
                    WHERE id = %s
                """, (best_inc_id, float(final_score), int(my_id)))
                
                # ì‚¬ê±´ ìƒíƒœ ê°±ì‹  (OPEN ìœ ì§€/ì „í™˜)
                cursor.execute("""
                    UPDATE incidents 
                    SET complaint_count = complaint_count + 1, status = 'OPEN' 
                    WHERE id = %s
                """, (best_inc_id,))
                
                logging.info(f"   ğŸ‰ [ë³‘í•© ì™„ë£Œ] ë¯¼ì› #{my_id} -> ì‚¬ê±´ #{best_inc_id}")
                merged_ids.append(my_id)

        except Exception as e:
            # ë²¡í„° í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆê±°ë‚˜ pgvectorê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥
            logging.error(f"   âŒ SQL ì‹¤í–‰ ì—ëŸ¬: {e}")
            conn.rollback() 
        
        time.sleep(0.1)

    conn.commit()
    cursor.close()
    
    # ë³‘í•©ë˜ì§€ ì•Šì€ ë‚˜ë¨¸ì§€ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
    return new_df[~new_df['id'].isin(merged_ids)]

# ==========================================
# 4. ì‹ ê·œ êµ°ì§‘ ìƒì„± (ë‚¨ì€ ê²ƒë“¤ë¼ë¦¬ ë­‰ì¹˜ê¸°)
# ==========================================

def cluster_remaining_complaints(conn, df):
    if df.empty: return

    logging.info(f"ğŸ§© [ì‹ ê·œ êµ°ì§‘í™”] ë‚¨ì€ ë¯¼ì› {len(df)}ê±´ ì²˜ë¦¬ ì¤‘...")
    cursor = conn.cursor()
    
    df['district_id'] = df['district_id'].fillna(0)
    grouped = df.groupby('district_id')

    for dist_id, group in grouped:
        if len(group) < 2:
            # ë‹¨ë… ë¯¼ì› (Noise)
            save_incident(cursor, group, is_noise=True)
            continue

        embeddings = np.array([parse_embedding(e) for e in group['embedding']])
        keywords_list = [k if k else [] for k in group['keywords_jsonb'].tolist()]
        
        # ì—¬ê¸°ì„œëŠ” ì—¬ì „íˆ DBSCAN ì‚¬ìš© (ìš°ë¦¬ë¼ë¦¬ ë­‰ì¹  ë•ŒëŠ” ì´ê²Œ ìµœê³ )
        l1_dist = calculate_hybrid_distance(embeddings, keywords_list, alpha=0.6)
        l1_labels = DBSCAN(eps=0.2, min_samples=2, metric='precomputed').fit_predict(l1_dist)

        for l1_lab in set(l1_labels):
            l1_indices = np.where(l1_labels == l1_lab)[0]
            l1_df = group.iloc[l1_indices]

            if l1_lab == -1: 
                save_incident(cursor, l1_df, is_noise=True)
            else:
                save_incident(cursor, l1_df, is_noise=False)

    conn.commit()
    cursor.close()

def save_incident(cursor, df, is_noise=False):
    if df.empty: return

    iterator = df.iterrows() if is_noise else [(None, df)]
    
    for _, row_data in iterator:
        if is_noise:
            target_df = pd.DataFrame([row_data])
            row_item = row_data
            count = 1
        else:
            target_df = row_data
            row_item = target_df.iloc[0]
            count = len(target_df)

        dist_name = row_item['district_name'] if row_item['district_name'] else "ì„œìš¸ì‹œ"
        
        all_k = []
        for k_list in target_df['keywords_jsonb']:
            if k_list: all_k.extend(k_list)
        top_k = Counter(all_k).most_common(5)
        
        main_keyword = top_k[0][0] if top_k else "ë¯¼ì›"
        keywords_str = ", ".join([k[0] for k in top_k]) 
        
        valid_reqs = [r for r in target_df['core_request'].tolist() if r]
        raw_summ = max(valid_reqs, key=len) if valid_reqs else "ë‚´ìš© ì—†ìŒ"
        
        title = f"{dist_name} {main_keyword} ê´€ë ¨ {raw_summ}"
        title = clean_text_for_title(title)[:100].strip()

        d_id = int(row_item['district_id']) if row_item['district_id'] > 0 else None
        
        try:
            if is_noise:
                # ë…¸ì´ì¦ˆëŠ” ì €ì¥ ì•ˆ í•¨ (í•„ìš” ì‹œ ì£¼ì„ í•´ì œ)
                pass
            else:
                cursor.execute("""
                    INSERT INTO incidents (title, status, complaint_count, keywords, district_id, opened_at)
                    VALUES (%s, 'OPEN', %s, %s, %s, NOW())
                    RETURNING id
                """, (title, count, keywords_str, d_id))
                inc_id = cursor.fetchone()[0]

                ids = tuple(target_df['id'].tolist())
                cursor.execute(f"""
                    UPDATE complaints 
                    SET incident_id = %s, incident_linked_at = NOW(), incident_link_score = 0.95 
                    WHERE id IN %s
                """, (inc_id, ids))
                
                logging.info(f"   ğŸ†• [ìƒˆ ì‚¬ê±´ ìƒì„±] #{inc_id} : {title} ({count}ê±´)")
        except Exception as e:
            logging.error(f"   âŒ ì‚¬ê±´ ì €ì¥ ì‹¤íŒ¨: {e}")

# ==========================================
# 5. ìƒíƒœ ë™ê¸°í™”
# ==========================================

def sync_incident_status(conn):
    cursor = conn.cursor()
    try:
        # OPEN -> CLOSED (ëª¨ë“  ë¯¼ì›ì´ ì¢…ë£Œë˜ë©´)
        cursor.execute("""
            UPDATE incidents i
            SET status = 'CLOSED', closed_at = NOW()
            WHERE i.status = 'OPEN'
            AND NOT EXISTS (
                SELECT 1 FROM complaints c 
                WHERE c.incident_id = i.id 
                AND c.status NOT IN ('CLOSED', 'CANCELED')
            )
            AND EXISTS (SELECT 1 FROM complaints c WHERE c.incident_id = i.id)
        """)
        if cursor.rowcount > 0:
            logging.info(f"   ğŸ [ìƒíƒœ ë™ê¸°í™”] {cursor.rowcount}ê°œ ì‚¬ê±´ ìë™ ì¢…ê²°")

        conn.commit()
    except Exception as e:
        logging.error(f"ìƒíƒœ ë™ê¸°í™” ì—ëŸ¬: {e}")
        conn.rollback()
    finally:
        cursor.close()

# ==========================================
# 6. ë©”ì¸ ì‹¤í–‰ ë£¨í”„
# ==========================================

def run_daily_job():
    with engine.begin() as conn:
        try:
            # 1. ì‹ ê·œ ë¯¼ì› ì¡°íšŒ (ì•„ì§ ì‚¬ê±´ ë²ˆí˜¸ ì—†ëŠ” ê²ƒ)
            sql = """
                SELECT n.complaint_id as id, n.core_request, n.embedding,
                    n.keywords_jsonb, n.district_id, n.target_object, 
                    d.name as district_name
                FROM complaint_normalizations n
                JOIN complaints c ON n.complaint_id = c.id
                LEFT JOIN districts d ON n.district_id = d.id
                WHERE c.incident_id IS NULL 
                LIMIT 100        
            """
            new_df = pd.read_sql(sql, conn)
            new_df['district_id'] = new_df['district_id'].fillna(0)

            if not new_df.empty:
                logging.info(f"ğŸš€ ì‹ ê·œ ë¯¼ì› {len(new_df)}ê±´ ê°ì§€ ë° ì²˜ë¦¬ ì‹œì‘")
                
                # psycopg2 ì „ìš© ë¡œì§ì´ í•„ìš”í•˜ë‹¤ë©´ raw connection í™œìš©
                raw_conn = conn.connection
                
                # [Step 1] í•˜ì´ë¸Œë¦¬ë“œ ë³‘í•©
                remaining_df = try_merge_to_existing_incidents_hybrid(raw_conn, new_df)
                
                # [Step 2] ì‹ ê·œ êµ°ì§‘í™”
                if not remaining_df.empty:
                    cluster_remaining_complaints(raw_conn, remaining_df)
                
                logging.info("âœ… ì£¼ê¸°ì  êµ°ì§‘í™” ì‘ì—… ì™„ë£Œ")
            
            # ìƒíƒœ ë™ê¸°í™”
            sync_incident_status(raw_conn)

        except Exception as e:
            logging.error(f"âŒ ì‘ì—… ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

def wait_interval(duration):
    # logging.debug(f"{duration}ì´ˆ ëŒ€ê¸° ì¤‘...") # êµ³ì´ ì•ˆ ë‚¨ê²¨ë„ ë¨
    time.sleep(duration)

if __name__ == "__main__":
    logging.info("ğŸ¤– [Hybrid Cluster] ì„œë²„ ì„œë¹„ìŠ¤ ì‹œì‘")
    logging.info(f"   - í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê¸°ì¤€: {HYBRID_THRESHOLD}ì ")
    print("="*60 + "\n")

    while True:
        run_daily_job()
        wait_interval(CHECK_INTERVAL)