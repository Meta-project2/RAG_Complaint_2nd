import ast
import os
import numpy as np
import pandas as pd
import psycopg2
from psycopg2.extras import Json
import requests
from datetime import datetime

# --- ì„¤ì • ì„¹ì…˜ ---
DB_CONFIG = {
    "host": "localhost",
    "database": "postgres",
    "user": "postgres",
    "password": "0000",
    "port": 5432
}

OLLAMA_URL = "http://localhost:11434/api/embeddings"
EMBED_MODEL = "mxbai-embed-large"
base_path = os.path.dirname(os.path.abspath(__file__))
CSV_FILE = os.path.join(base_path, "ê°•ë™êµ¬_structured_final.csv")
TABLE_NAME = "complaint_normalizations"

def get_embedding(text):
    payload = {"model": EMBED_MODEL, "prompt": f"doc: {text}"}
    try:
        res = requests.post(OLLAMA_URL, json=payload, timeout=10)
        return res.json()['embedding']
    except Exception as e:
        print(f"Embedding Error: {e}")
        return None
    
def clean_keywords(raw_value):
    if pd.isna(raw_value) or str(raw_value).strip() == "":
        return []
    try:
        return ast.literal_eval(str(raw_value))
    except (ValueError, SyntaxError):
        return [k.strip() for k in str(raw_value).split(',')]

def migrate_data():
    try:
        df = pd.read_csv(CSV_FILE, encoding='utf-8-sig')
    except:
        df = pd.read_csv(CSV_FILE, encoding='cp949')

    # CSV ì½ì„ ë•Œ ë‚ ì§œ ë³€í™˜ ë¯¸ë¦¬ ì ìš© (ì—ëŸ¬ ë°©ì§€)
    df['req_date'] = pd.to_datetime(df['req_date'], errors='coerce')
    df['resp_date'] = pd.to_datetime(df['resp_date'], errors='coerce')

    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()

    cur.execute(f"SELECT COUNT(*) FROM {TABLE_NAME}")
    last_count = cur.fetchone()[0]
    
    print(f"í˜„ì¬ DB({TABLE_NAME})ì— ì €ì¥ëœ ë°ì´í„° ìˆ˜: {last_count}ê±´")

    df_to_process = df.iloc[last_count:]
    df_to_process = df_to_process.replace({np.nan: None})
    
    if len(df_to_process) == 0:
        print("âœ¨ ì´ë¯¸ ëª¨ë“  ë°ì´í„°ê°€ ì´ê´€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸš€ ì´ {len(df)}ê±´ ì¤‘ {last_count}ê±´ ì´í›„ì¸ {len(df_to_process)}ê±´ë¶€í„° ì´ê´€ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    for i, row in df_to_process.iterrows():
        try:
            # 1. ë¶€ëª¨ í…Œì´ë¸” ì‚½ì…
            # req_date (ì ‘ìˆ˜ì¼) -> received_at, created_at
            # resp_date (ë‹µë³€ì¼) -> closed_at, updated_at (ë‹µë³€ì¼ ì—†ìœ¼ë©´ ì ‘ìˆ˜ì¼ë¡œ updated_at ì±„ì›€)
            req_time = row['req_date']
            resp_time = row['resp_date'] if pd.notnull(row['resp_date']) else None
            
            # ìƒíƒœ ê²°ì •: ë‹µë³€ì¼ ìˆìœ¼ë©´ CLOSED, ì—†ìœ¼ë©´ RECEIVED
            status = 'CLOSED' if resp_time else 'RECEIVED'
            
            sql_parent = """
            INSERT INTO complaints (
                received_at, title, body, answer, district_id, status, address_text, 
                created_at, updated_at, closed_at, 
                current_department_id, applicant_id, tag
            ) VALUES (%s, %s, %s, %s, 2, %s, %s, %s, %s, %s, 3, 1, 'OTHER') RETURNING id;
            """
            
            cur.execute(sql_parent, (
                req_time,               # received_at
                row['req_title'], 
                row['req_content'], 
                row['resp_content'],
                status,                 # status
                row['resp_dept'],       # address_text
                req_time,               # created_at
                resp_time if resp_time else req_time, # updated_at
                resp_time               # closed_at
            ))
            new_complaint_id = cur.fetchone()[0]

            # 2. ì„ë² ë”© ìƒì„±
            vector = get_embedding(row['search_text'])
            if not vector:
                print(f"âš ï¸ [{i}] ì„ë² ë”© ì‹¤íŒ¨ - ì´ í–‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                conn.rollback()
                continue

            # 3. ìì‹ í…Œì´ë¸” ì‚½ì… (ìˆ˜ì •ëœ ë¶€ë¶„)
            # [ìˆ˜ì •] created_at ì»¬ëŸ¼ì„ ì¶”ê°€í•˜ì—¬ DB ê¸°ë³¸ê°’(2026ë…„) ëŒ€ì‹  CSV ë‚ ì§œê°€ ë“¤ì–´ê°€ë„ë¡ ë³€ê²½
            sql_child = """
            INSERT INTO complaint_normalizations (
                complaint_id, neutral_summary, core_request, 
                target_object, keywords_jsonb, embedding, resp_dept,
                created_at  -- â˜… ë‚ ì§œ ì»¬ëŸ¼ ì¶”ê°€
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            keywords_list = clean_keywords(row['keywords'])
            
            cur.execute(sql_child, (
                new_complaint_id,
                row['search_text'],
                row['topic'],
                row['category'],
                Json(keywords_list),
                vector,
                row['resp_dept'],
                req_time  # â˜… created_atì— ì ‘ìˆ˜ì¼ ì‚¬ìš©
            ))

            # 4. ê°œë³„ ê±´ë³„ ì»¤ë°‹
            conn.commit()
            
            if (i + 1) % 10 == 0 or i == len(df) - 1:
                print(f"âœ… [{i+1}/{len(df)}] ì´ê´€ ì™„ë£Œ (ID: {new_complaint_id}) - ë‚ ì§œ: {req_time}")

        except Exception as e:
            conn.rollback()
            print(f"âŒ Error at row {i}: {e}")
            break 

    cur.close()
    conn.close()
    print("âœ¨ ì´ê´€ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")

if __name__ == "__main__":
    migrate_data()
