import os
import pandas as pd
from sqlalchemy import create_engine, text
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision
from ragas.llms import llm_factory
from ragas.embeddings import embedding_factory
from openai import OpenAI
from datasets import Dataset

# ==========================================
# 1. í™˜ê²½ ì„¤ì •
# ==========================================
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

DB_CONFIG = {
    "host": "34.50.48.38",
    "database": "postgres",
    "user": "postgres",
    "password": "0000",
    "port": 5432
}

def format_to_sentence(data):
    return (
        f"ì†Œê´€ ë¶€ì„œ: [{data['dept']}], "
        f"ì‚¬ë¡€ ìš”ì•½: {data['summary']}, "
        f"í•µì‹¬ í‚¤ì›Œë“œ: {data['keywords']}, "
        f"ë„ë©”ì¸ ì¹´í…Œê³ ë¦¬: {data['category']}"
    )

def get_filtered_evaluation_dataset():
    db_url = f"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}"
    engine = create_engine(db_url)
    
    target_query = text("""
        SELECT c.id, c.body AS question, n.resp_dept, n.neutral_summary,
               n.keywords_jsonb, n.target_object, n.embedding
        FROM complaints c
        JOIN complaint_normalizations n ON c.id = n.complaint_id
        WHERE n.resp_dept IS NOT NULL
        LIMIT 100
    """)
    
    eval_rows = []
    with engine.connect() as conn:
        targets = conn.execute(target_query).fetchall()
        
        for t in targets:
            category = t.target_object or ""
            cat_pattern = "|".join([category[i:i+2] for i in range(len(category)-1)]) if len(category) >= 2 else category
            emb_str = str(t.embedding) if isinstance(t.embedding, list) else t.embedding

            # [í•´ê²°] SELECT ì ˆì— sub.target_objectë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.
            search_query = text("""
                SELECT 
                    sub.resp_dept, 
                    sub.neutral_summary, 
                    sub.keywords_jsonb, 
                    sub.target_object,  -- ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì–´ ì—ëŸ¬ê°€ ë°œìƒí–ˆì—ˆìŠµë‹ˆë‹¤.
                    sub.final_score
                FROM (
                    SELECT cn.*,
                        ((1 - (cn.embedding <=> CAST(:emb AS vector))) * 0.6 + 
                         ts_rank(cn.search_vector, plainto_tsquery('simple', :keywords)) * 0.2 + 
                         (CASE WHEN cn.resp_dept::text ~ :cat_pattern THEN 0.2 ELSE 0 END)) AS final_score
                    FROM complaint_normalizations cn
                    WHERE cn.complaint_id != :tid
                ) sub
                WHERE sub.final_score > 0.45
                ORDER BY sub.final_score DESC
                LIMIT 1
            """)
            
            keywords_str = " ".join(t.keywords_jsonb) if isinstance(t.keywords_jsonb, list) else ""
            
            res = conn.execute(search_query, {
                "emb": emb_str, 
                "keywords": keywords_str,
                "cat_pattern": cat_pattern,
                "tid": t.id
            }).fetchone()
            
            if res:
                actual_data = {
                    'dept': t.resp_dept,
                    'summary': t.neutral_summary,
                    'keywords': ", ".join(t.keywords_jsonb) if isinstance(t.keywords_jsonb, list) else "",
                    'category': t.target_object
                }
                
                predict_data = {
                    'dept': res.resp_dept,
                    'summary': res.neutral_summary,
                    'keywords': ", ".join(res.keywords_jsonb) if isinstance(res.keywords_jsonb, list) else "",
                    'category': res.target_object  # ì´ì œ ì •ìƒì ìœ¼ë¡œ ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.
                }
                
                eval_rows.append({
                    "question": t.question,
                    "ground_truth": format_to_sentence(actual_data),
                    "answer": format_to_sentence(predict_data),
                    "contexts": [res.neutral_summary]
                })

    return Dataset.from_pandas(pd.DataFrame(eval_rows))

def run_evaluation(dataset):
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    eval_llm = llm_factory("gpt-4o", client=openai_client)
    eval_embeddings = embedding_factory(
        "openai", 
        model="text-embedding-3-large", 
        client=openai_client, 
        interface="modern",
    )
    metrics = [
        faithfulness,
        answer_relevancy,
        context_recall,
        context_precision
    ]

    print(f"ğŸ“Š Ragas í‰ê°€ ì§€í‘œ ê³„ì‚° ì‹œì‘ (ì´ {len(dataset)}ê±´)...")

    results = evaluate(
        dataset=dataset,
        metrics=metrics,
        llm=eval_llm,    
        embeddings=eval_embeddings
    )
    
    results.to_pandas().to_csv("ragas_eval_results.csv", index=False, encoding='utf-8-sig')
    print("âœ… í‰ê°€ ì™„ë£Œ! 'ragas_eval_results.csv' íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return results

if __name__ == "__main__":
    try:
        ds = get_filtered_evaluation_dataset()
        if ds and len(ds) > 0:
            print(f"âœ… {len(ds)}ê±´ì˜ ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤. í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
            results = run_evaluation(ds)
            print("\nâœ¨ [ìµœì¢… ê²°ê³¼]")
            print(results)
        else:
            print("âš ï¸ ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")